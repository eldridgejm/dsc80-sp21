{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSC 80: Project 05\n",
    "\n",
    "### Due Date: Monday, June 7, 11:59:59 PM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions\n",
    "\n",
    "This project will be an open-ended investigation into the dataset you chose for project 3; you will pose a prediction problem and train a model to solve it. You will follow the steps given below:\n",
    "\n",
    "1. Use the dataset you chose in project 03 (`nypd`, `github`, `outages`). Clearly state and frame a prediction problem (classification or regression); choose and justify an objective (e.g. accuracy vs f1-score).\n",
    "1. Train a \"baseline\" model with generic set of features created for different kinds of data. You do not need to use *every* feature but please at least use a couple. You can leave the numerical ones as is but you'll need to take care of the string columns (e.g. ordinal encoding, one-hot encoding). **Everything should be done in an `sklearn` `Pipeline`**.\n",
    "1. Engineer at least two new features from the data (e.g. StandardScaler from lab09, binning/binarizing data, etc) that improve your baseline model and create a \"final\" `sklearn` `Pipeline`. Also, perform a search for the best model and parameters using the pipeline (i.e. using `GridSearchCV` or some manual iterative method.\n",
    "1. Do an \"fairness\" analysis on the results (i.e. does my model perform better on attribute X vs Y?) More on this later.\n",
    "\n",
    "\n",
    "## How to Organize and Submit Your Work for Grading\n",
    "\n",
    "### Summary of Findings\n",
    "\n",
    "You must summarize your results in a *Summary  of Findings* section, in markdown cells. **Failure to summarize your results this way will result in a zero.**\n",
    "\n",
    "The *Summary of Findings* section should include a summary of each of the above bullets in the project description above:\n",
    "1. State the *prediction problem* you are attempting. State whether the problem is a classification or regression problem. Explain your choice of target variable and evaluation metric (objective).\n",
    "1. A summary of the baseline model:\n",
    "    - The number of features, including how many are quantitative, ordinal, and nominal.\n",
    "    - The model performance (your evaluation metric) and whether you think this it's good or not (and why).\n",
    "1. A summary of the final, improved model:\n",
    "    - The features you added and *why* they are good for your data.\n",
    "    - The model type you chose; the parameters that ended up performing best; the method of model selection used.\n",
    "1. Evaluate your model for \"fairness\" on an interesting subset of the data using a permutation test. Justify the parity measure you are using. I find this easiest to illustrate with an example:\n",
    "\n",
    "Say we have a sample voter dataset with columns `Name`, `Age`, `Voted`, etc, and you are trying to predict whether or not someone voted (1 means yes, 0 means no).\n",
    "\n",
    "For this example, I'll choose `Age` to construct my \"interesting subset\": let's say I want to explore whether my model is fairer for younger or older people. I can binarize my subset by putting an age threshold at 40, deeming everyone younger than 40 as \"young\" and everyone older as \"old.\" Now my dataset is split into subsets: a young subset and an old subset. These are my X and Y.\n",
    "\n",
    "For my parity measure, I will pick precision (check lecture 17 for more measures to pick from, think of which are more appropriate for which cases!).\n",
    "\n",
    "Now for the permutation test. We need a null and an alternative hypothesis. \n",
    "\n",
    "- Null Hypothesis: my model is fair; the precision for my two subsets are roughly the same\n",
    "\n",
    "- Alternative Hypothesis: my model is unfair; the precision for the old subset is higher than the young subset\n",
    "\n",
    "I'll leave you at this, see if you can figure out how to actually perform (code) the permutation test. Good luck!\n",
    "\n",
    "*Note: if you are doing a regression problem, you can't use the typical \"fairness\" metrics from class (precision, recall, etc). You will have to use either RMSE or $R^2$.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grading\n",
    "**You will be graded on addressing the above bullet points in your summary.** You should have your code included at bottom in a clearly done, commented fashion. Upon reading the summary, it should be easy for the grader to glance down at the code to see the supporting work.\n",
    "\n",
    "You may keep your code in a `.py` file and import it if you'd like. Just (1) be sure the output is clearly visible in the notebook, and (2) copy and paste the code from the file to the *very* bottom of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submitting Your Project\n",
    "\n",
    "You will submit the notebook with your write-up (and code) to Gradescope; you will **not** be turning in a `.py` file.\n",
    "\n",
    "1. Save your notebook (e.g. `nypd.ipynb`) as a PDF file. To do this, use your web browser to print the page; choose the print to PDF option (this is more reliable).\n",
    "2. Upload the file to the Gradescope assignment for the dataset you chose. For example, if you did `nypd`, then upload your pdf to the assignment `project05_nypd`.\n",
    "3. You are done! (You do *not* need to upload this notebook)."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
